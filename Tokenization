#Tokenization code in googlecolab

!pip install nltk

# Import NLTK
import nltk
nltk.download('punkt')  # Download necessary punkt tokenizer data

# Define sample text for analysis
text = " Because they connect the “small” ideas of individual sentences to a “bigger” idea, paragraph structure is essential to any writing for organization, flow, and comprehension. "

# Sentence tokenization
from nltk.tokenize import sent_tokenize
sent_tokenize_list = sent_tokenize(text)

# Print the list of sentences
print("\nSentence tokenizer:")
print(sent_tokenize_list)

# Word tokenization (basic word tokenizer)
from nltk.tokenize import word_tokenize
print("\nWord tokenizer:")
print(word_tokenize(text))

# Punkt word tokenizer
from nltk.tokenize import WordPunctTokenizer
punkt_word_tokenizer = WordPunctTokenizer()
print("\nPunkt word tokenizer:")
print(punkt_word_tokenizer.tokenize(text))

# WordPunct tokenizer
from nltk.tokenize import WordPunctTokenizer
word_punct_tokenizer = WordPunctTokenizer()
print("\nWord punct tokenizer:")
print(word_punct_tokenizer.tokenize(text))
